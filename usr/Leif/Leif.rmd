---
title: "Untitled"
author: "Leif Watkins"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)  # Load core packages
library(MASS)       # LDA, QDA, OLS, Ridge Regression, Box-Cox, stepAIC, etc,.
library(e1071)      # Naive Bayesian Classfier,SVM, GKNN, ICA, LCA
library(class)      # KNN, SOM, LVQ
library(ROCR)       # Precision/Recall/Sensitivity/Specificity performance plot 
library(boot)       # LOOCV, Bootstrap,
set.seed(1234)
```



#get the white wine data and clean it . 
```{r}
data <- read.csv("data\\winequality-white.csv", sep = ";", header = T)

# this is to convert the quality predictor from numerical into categorical
wine_data <-  data %>% mutate(good = ifelse(quality>=7, 1, 0))
wine_data$quality <- NULL

wine_data <- wine_data[!duplicated(wine_data), ]

head(wine_data)
```

#looking in to the data we are working with 
```{r}
str(wine_data)


### spiting the data into train and test sets


### Data Split
#splitting the dataset into train and test (2/3rd for train remaining for test)
inTrain <- caret::createDataPartition(wine_data$good, p = 7/10, list = F)
train <- wine_data[inTrain,]
test <- wine_data[-inTrain,]
wine_data <- wine_data[!duplicated(wine_data), ]

train$good <- as.factor(train$good)
test$good <- as.factor(test$good)

```

# fitting a logistic regression 
```{r}

#wine_glm<- glm(good ~ . ,wine_data)
#summary(wine_glm)

#w2_glm <- glm(good ~ . -citric.acid, data = wine_data )
#w2_glm <- logit(good ~ . -citric.acid, data = wine_data )
#w2_glm$residuals
#summary(w2_glm)

```




### qda 
```{r}

### THE OLD WAY OF DOING IT
set.seed(1234)
qda.fit = qda(good ~ . , data = train, family = binomial)
qda.fit
# 
# 
set.seed(1234)
qda.pred  = predict(qda.fit, test)
names(qda.pred)
qda.class = qda.pred$class
length(test$good)
length(qda.class)
#      truth   , prediction
#table(qda.class, test$good )
table(test$good, qda.class )

#caret::confusionMatrix(qda.class, test$good )

#### using caret package
library(caret)

set.seed(1234)
train_qda <- trainControl(method = "cv", number = 10 )
set.seed(1234)
wine_qda <- train(good ~ . , data = train , method = "qda", trControl = train_qda)

#predictions
qda_pred <- predict(wine_qda, newdata= test)
caret::confusionMatrix(test$good ,qda_pred)



# # ROCR #
# qda.pred <- prediction(qda.pred$posterior[,2], Smarket.2005$Direction) 
# qda.perf <- performance(qda.pred,"tpr","fpr")
# plot(qda.perf,colorize=TRUE, lwd = 2)
# abline(a = 0, b = 1) 
# 
# 
# qda.auc = performance(qda.pred, measure = "auc")
# print(qda.auc@y.values)
# 
# 
# results.matrix[3,] = as.numeric( c(spec.qda, sen.qda, oer.qda1, qda.auc@y.values))
# 
# 

```



#### LDA  
```{r}


#Another way to split the original data into training and testing sets
set.seed(1234) # set seed for reproducibility
trainIndex <- sample(1:nrow(wine_data), round(0.7*nrow(wine_data)), replace=FALSE)
train <- data[trainIndex, ]
test <- data[-trainIndex, ]

# Set up 10-fold cross-validation
folds <- createFolds(train, k = 10, returnTrain = TRUE)

# Set up a vector to store the accuracy of each fold
accuracy <- numeric(10)

# Loop through each fold
for (i in 1:10)
{
  # Get the indices for the training and validation sets for this fold
  trainIndex <- unlist(folds[i])
  validIndex <- setdiff(1:nrow(train), trainIndex)
  
  # Train the logistic regression model on the training set for this fold
  lda.fit <- lda(train$good ~ . , family = binomial, data = train[trainIndex,])
  #log.reg <- glm(good ~ ., family = binomial, data = train[trainIndex, ])
  
  # Make predictions on the validation set for this fold
  log.pred <- predict(lda.fit, train[validIndex, ], type = "response")
  log.pred <- factor(ifelse(log.pred > 0.7, 1, 0), levels = c(0, 1))
  
  # Calculate the accuracy of the predictions for this fold
  accuracy[i] <- mean(train$good[validIndex] == log.pred)
}

# Calculate the average accuracy across all folds
mean.accuracy <- mean(accuracy)

# Train the logistic regression model on the entire training set using the optimal hyperparameters
lda.fit = lda(good ~ . , data = train, family = binomial)
#log.reg <- glm(good ~ ., family = binomial, data = train)

# Make predictions on the testing set using the trained logistic regression model
log.pred <- predict(lda.fit, test, type = "response")
log.pred <- factor(ifelse(log.pred) > 0.7, 1, 0), levels = c(0, 1))

# Check the confusion matrix 
length(log.pred)
length(test$good)
confusionMatrix(log.pred, test$good)

#AUC
log.pred <- as.numeric(log.pred)
log.pred <- prediction(log.pred, test$good)
log.perf <- performance(log.pred,"tpr","fpr")
plot(log.perf,colorize=TRUE, lwd = 2)
abline(a = 0, b = 1)
log.auc = performance(log.pred, measure = "auc")
print(log.auc@y.values)

```

