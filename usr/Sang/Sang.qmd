---
title: "Introduction to Statistical Learning"
subtitle: "STAT 387" 
author: "Sang Xing"
format: 
  html:
    df-print: paged
    theme:
      light: materia
      dark: slate
    toc: true
    toc-title: Table of Contents
    toc-depth: 4
    toc-location: right
    number-sections: false
    number-depth: 3
    anchor-sections: true
    smooth-scroll: true
    link-external-icon: false
    link-external-newwindow: true
    code-fold: false
    code-overflow: scroll
    code-tools: 
      source: true
      toggle: true
      caption: none
    code-summary: "Show the code"
    highlight-style: atom-one
    link-external-filter: '^(?:http:|https:)\/\/www\.quarto\.org\/custom'
    embed-resources: true
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

\vspace{-1cm}

```{r setup, include=FALSE}
library(tidyverse)  # Load core packages: 
# ggplot2,   for data visualization.
# dplyr,     for data manipulation.
# tidyr,     for data tidying.
# purrr,     for functional programming.
# tibble,    for tibbles, a modern re-imagining of data frames.
# stringr,   for strings.
# forcats,   for factors.
# lubridate, for date/times.
# readr,     for reading .csv, .tsv, and .fwf files.
# readxl,    for reading .xls, and .xlxs files.
# feather,   for sharing with Python and other languages.
# haven,     for SPSS, SAS and Stata files.
# httr,      for web apis.
# jsonlite   for JSON.
# rvest,     for web scraping.
# xml2,      for XML.
# modelr,    for modelling within a pipeline
# broom,     for turning models into tidy data
# hms,       for times.

library(magrittr)   # Pipeline operator
library(lobstr)     # Visualizing abstract syntax trees, stack trees, and object sizes
library(pander)     # Exporting/converting complex pandoc documents, EX: df to Pandoc table
library(ggforce)    # More plot functions on top of ggplot2
library(ggpubr)     # Automatically add p-values and significance levels  plots. 
# Arrange and annotate multiple plots on the same page. 
# Change graphical parameters such as colors and labels.
library(sf)         # Geo-spatial vector manipulation: points, lines, polygons
library(kableExtra) # Generate 90 % of complex/advanced/self-customized/beautiful tables
library(latex2exp)  # Latex axis titles in ggplot2
library(ellipse)    # Simultaneous confidence interval region to check C.I. of 2 slope parameters
library(plotly)     # User interactive plots
library(ellipse)    # Simultaneous confidence interval region to check C.I. of 2 regressors
library(olsrr)      # Model selections 
library(leaps)      # Regression subsetting 
library(pls)        # Partial Least squares
library(MASS)       # LDA, QDA, OLS, Ridge Regression, Box-Cox, stepAIC, etc,.
library(e1071)      # Naive Bayesian Classfier,SVM, GKNN, ICA, LCA
library(class)      # KNN, SOM, LVQ
library(ROCR)       # Precision/Recall/Sensitivity/Specificity performance plot 
library(boot)       # LOOCV, Bootstrap,
library(caret)      # Classification/Regression Training, run ?caret::trainControl
library(corrgram)   # for correlation matrix
library(corrplot)   # for graphical display of correlation matrix

set.seed(1234)        # make random results reproducible

current_dir <- getwd()

if (!is.null(current_dir)) {
  setwd(current_dir)
  remove(current_dir)
}
```

```{css, echo=FALSE}
body {
font-family: "Times New Roman";
font-size: 0.8em;
}

h2, .h2, h2.anchored, 
h3, .h3, h3.anchored,
.quarto-title, 
.quarto-title-meta {
text-align: center;
}

h2, .h2 {
font-size: 1.5em;
}

h3, .h3 {
font-size: 1.25em;
}

h4, .h4 {
font-size: 1em;
}

h5, .h5 {
font-size: 0.75em;
}

h6, .h6 {
font-size: 0.7em;
}

.quarto-title-meta {
display: block!important;
}

.quarto-title-meta-heading {
display: none;
}

code {
color: #61AFEF;
}

.btn {
border-color: #f8f9fa9e!important;
}

body.quarto-dark {
background-color: #282c34;

color: #d8dee9;

.pagedtable .even {
background-color: #3b4252!important;
}

p code:not(.sourceCode), li code:not(.sourceCode) {
background-color: #4c566a!important;
}
}
```


## Preamble

Consider the wine quality dataset from [UCI Machine Learning Respository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality). We will focus only on the data concerning white wines (and not red wines). Dichotomize the `quality` variable as `good`, which takes the value 1 if quality â‰¥ 7 and the value 0, otherwise. We will take `good` as response and all the 11 physiochemical characteristics of the wines in the data as predictors.

### Problem Statements

Use 10-fold cross-validation for estimating the test error rates below and compute the estimates using `caret` package with seed set to 1234 before each computation.

(a) Fit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.
(b) Repeat (a) using logistic regression.
(c) Repeat (a) using LDA.
(d) Repeat (a) using QDA.
(e) Compare the results in (a)-(d). Which classifier would you recommend? Justify your answer.



### Methodologies
- KNN
- GLM/logit/glmnet
- LDA
- QDA
- Naive Bayes
- Random Forest
- Decision Tree
- Regression Tree
- SVM
- Regression/Multi-Regression

### Data Description
This is a dataset of wine quality containing 4898 observations of 12 variables. The variables are:

- `fixed.acidity`: The amount of fixed acid in the wine ($g/dm^3$)
- `volatile.acidity`: The amount of volatile acid in the wine ($g/dm^4$)
- `citric.acid`: The amount of citric acid in the wine ($g/dm^3$)
- `residual.sugar`: The amount of residual sugar in the wine ($g/dm^3$)
- `chlorides`: The amount of salt in the wine ($g/dm^3$)
- `free.sulfur.dioxide`: The amount of free sulfur dioxide in the wine ($mg/dm^3$)
- `total.sulfur.dioxide`: The amount of total sulfur dioxide in the wine ($mg/dm^3$)
- `density`: The density of the wine ($g/dm^3$)
- `pH`: The $pH$ value of the wine
- `sulphates`: The amount of sulphates in the wine ($g/dm^3$)
- `alcohol`: The alcohol content of the wine ($\% vol$)
- `quality`: The quality score of the wine ($0-10$)

After removing the duplicate rows from our data set, we are left with 3961 observations of the above 11 variables minus `quality` column variable, and introduced a new variable `good` as our response:

- `good`: A binary variable indicating whether the wine is good (`quality` $\geq 7$) or not (`quality` $<7$).


## Exploratory Analysis

### Data Import
```{r data_import}
wine.data <- read.csv("dataset\\winequality-white.csv", sep=";", header = T)

wine.data <-  wine.data %>% mutate(good = ifelse(quality>=7, 1, 0))

head(wine.data)
```

### Data Analysis
```{r basic_analysis}
dim(wine.data)

# Removing duplicate Rows
wine.data <- wine.data[!duplicated(wine.data), ]

dim(wine.data)

str(wine.data)
summary(wine.data)

# Check for NAs in dataset
sum(is.na(wine.data))

# Counts at each combination of response's factor levels
table(wine.data$quality)
```

### Data Histograms
```{r hist, warning=FALSE}
wine.colnames <- colnames(wine.data[, 1:12])
num_plots     <- length(wine.colnames)
num_rows      <- ceiling(num_plots/3)


# Create an empty list to store plots
grid_arr      <- list()


# Loop over each column name in the wine.colnames vector
for(i in 1:num_plots) {
  # Create a ggplot object for the current column using aes
  plt <- ggplot(data = wine.data, aes_string(x = wine.colnames[i])) +
    geom_histogram(binwidth = diff(range(wine.data[[wine.colnames[i]]]))/30, 
                   color = "black", fill = "slategray3") +
    labs(x = wine.colnames[i], y = "Frequency") +
    theme_bw()
  
  # Add the current plot to the grid_arr list
  grid_arr[[i]] <- plt
}

grid_arr <- do.call(gridExtra::grid.arrange, c(grid_arr, ncol = 3))
```

```{r, echo=FALSE}
# Remove unnecessary variables
remove(grid_arr)
remove(plt)
remove(i)
remove(num_plots)
remove(num_rows)
```

### Data Relationships
```{r relation_plot, fig.width=10, fig.height=5, message=FALSE}
reshape2::melt(wine.data[, 1:12], "quality") %>% 
  ggplot(aes(value, quality, color = variable)) +  
  geom_point() + 
  geom_smooth(aes(value,quality, colour=variable), method=lm, se=FALSE)+
  facet_wrap(.~variable, scales = "free")

# Collinearity between Attributes
cor(wine.data) %>% 
  corrplot::corrplot(method = 'number',  type = "lower", tl.col = "steelblue", number.cex = 0.5)

# Remove quality from dataset, use good as response 
wine.data$quality <- NULL 
```

### Data Split
```{r dataSplit}
# Splitting the dataset into train and test (7/10th for train remaining for test)
inTrain <- caret::createDataPartition(wine.data$good, p = 7/10, list = F)
train <- wine.data[inTrain,]
test <- wine.data[-inTrain,]
```

## K-Nearest Neightbor

### Model Construction
```{r modelSaving, eval=FALSE}
#--------------------#
#-----K-fold CV------#
#--------------------#

# Define the training control object for 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Train the KNN model using 10-fold cross-validation
# tuneLength argument to specify the range of values of K to be considered for tuning
knn_model <- train(good ~ ., 
                   data = train, 
                   method = "knn", 
                   trControl = train_control, 
                   tuneLength = 10)

# Save the model into .Rdata for future import 
save(knn_model, file = "dataset\\knn.model_kfoldCV.Rdata")


#--------------------#
#-------LOOCV--------#
#--------------------#

train_control <- trainControl(method = "LOOCV")

knn_model <- train(good ~ ., 
                   data = train, 
                   method = "knn", 
                   trControl = train_control)

save(knn_model, file = "dataset\\knn.model_looCV.Rdata")


#--------------------#
#----Hold-out CV-----#
#--------------------#

knn_model <- train(good ~ ., 
                   data = train, 
                   method = "knn", 
                   trControl = trainControl(method = "none"), 
                   tuneLength = 1)

save(knn_model, file = "dataset\\knn.model_holdoutCV.Rdata")

#--------------------#
#----Repeated CV-----#
#--------------------#

train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

kknn.grid <- expand.grid(kmax = c(3, 5, 7 ,9, 11), distance = c(1, 2, 3),
                         kernel = c("rectangular", "gaussian", "cos"))

knn_model <- train(good ~ ., data = train, method = "kknn",
                    trControl = train_control, tuneGrid = kknn.grid,
                    preProcess = c("center", "scale"))

save(knn_model, file = "dataset\\knn.model_repeatedCV.Rdata")
```


### K-fold CV 
```{r knn.kfoldCV}
set.seed(1234)
# Import model
load("dataset\\knn.model_kfoldCV.Rdata")

# Convert the outcome variable to a factor with two levels
train$good <- factor(train$good, levels = c(0, 1))
test$good <- factor(test$good, levels = c(0, 1))

# Make predictions on the test data using the trained model and calculate the test error rate
knn.predictions <- predict(knn_model, newdata = test)

confusionMatrix(knn.predictions, test$good)


# Convert predictions to a numeric vector
knn.predictions <- as.numeric(knn.predictions)

# Calculate the AUC using the performance() and auc() functions:
pred_obj <- prediction(knn.predictions, test$good)
auc_val <- performance(pred_obj, "auc")@y.values[[1]]
auc_val

# Performance plot
roc_obj <- performance(pred_obj, "tpr", "fpr")
plot(roc_obj,colorize=TRUE, lwd = 2)
points(auc_val, 1 - auc_val, col = "steelblue", pch = 21)
abline(a = 0, b = 1) 
```



### LOOCV
```{r knn.LOOCV}
load("dataset\\knn.model_looCV.Rdata")

knn.predictions <- predict(knn_model, newdata = test)

confusionMatrix(knn.predictions, test$good)

knn.predictions <- as.numeric(knn.predictions)
pred_obj <- prediction(knn.predictions, test$good)
auc_val <- performance(pred_obj, "auc")@y.values[[1]]
auc_val

roc_obj <- performance(pred_obj, "tpr", "fpr")
plot(roc_obj,colorize=TRUE, lwd = 2)
points(auc_val, 1 - auc_val, col = "steelblue", pch = 21)
abline(a = 0, b = 1) 
```


### Hold-out CV (Validation Set Approach)
```{r knn.holdoutCV}
load("dataset\\knn.model_holdoutCV.Rdata")

knn_model <- train(good ~ ., 
                   data = train, 
                   method = "knn", 
                   trControl = trainControl(method = "none"), 
                   tuneLength = 1)

knn.predictions <- predict(knn_model, newdata = test)

confusionMatrix(knn.predictions, test$good)

knn.predictions <- as.numeric(knn.predictions)
pred_obj <- prediction(knn.predictions, test$good)
auc_val <- performance(pred_obj, "auc")@y.values[[1]]
auc_val

roc_obj <- performance(pred_obj, "tpr", "fpr")
plot(roc_obj,colorize=TRUE, lwd = 2)
points(auc_val, 1 - auc_val, col = "steelblue", pch = 21)
abline(a = 0, b = 1) 
```


### Repeated CV
```{r knn.repeatedCV}
load("dataset\\knn.model_repeatedCV.Rdata")

knn.predictions <- predict(knn_model, newdata = test)

confusionMatrix(knn.predictions, test$good)

knn.predictions <- as.numeric(knn.predictions)
pred_obj <- prediction(knn.predictions, test$good)
auc_val <- performance(pred_obj, "auc")@y.values[[1]]
auc_val

roc_obj <- performance(pred_obj, "tpr", "fpr")
plot(roc_obj,colorize=TRUE, lwd = 2)
points(auc_val, 1 - auc_val, col = "steelblue", pch = 21)
abline(a = 0, b = 1) 
```

### Summary
| Resampling Method | Error Rate | Sensitivity | Specificity | AUC       |
| ----------------- | ---------- | ----------- | ----------- | --------- |
| K-Fold CV         | 0.1928     | 0.9831      | 0.1088      | 0.6896161 |
| LOOCV             | 0.1785     | 0.9557      | 0.2887      | 0.6222229 |
| Hold-out CV       | 0.2222     | 0.9083      | 0.2594      | 0.5838694 |
| Repeated CV       | 0.096      | 0.9621      | 0.6736      | 0.8178527 |

## Logistic Regression
```{r}
set.seed(1234)

```


## Linear Discriminant Analysis
```{r}
set.seed(1234)

```


## Quadratic discriminant analysis
```{r}
set.seed(1234)

```

