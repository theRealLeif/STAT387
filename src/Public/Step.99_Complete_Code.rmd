---
title: "STAT 387"
subtitle: "Introduction to Statistical Learning, Winter 2023" 
author: Alana, Leif, Sang
output: 
  rmdformats::downcute:
    highlight: tango
    code_folding: show  
    default_style: "dark"
    toc_depth: 3
df_print: kable   
---

```{css, echo=FALSE}
.title, .subtitle, .authors{
  text-align: center;
}

.glyphicon{
  display: none;
}

h1.subtitle {
    font-size: 1rem;
    font-weight: 100;
}

[data-theme='dark']
.page-content .code-mask, .page-content pre{
  background-color: #4d619657;
}

.page-content code{
  background: #4d619657;
  color: #fff;
}

[data-theme='light']
.page-content .code-mask, .page-content pre{
  background-color: #292c34;
}

.page-content code{
  background: #292c34;
  color: #fff;
}
```

\vspace{-1cm}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)  # Load core packages: 
                    # ggplot2,   for data visualization.
                    # dplyr,     for data manipulation.
                    # tidyr,     for data tidying.
                    # purrr,     for functional programming.
                    # tibble,    for tibbles, a modern re-imagining of data frames.
                    # stringr,   for strings.
                    # forcats,   for factors.
                    # lubridate, for date/times.
                    # readr,     for reading .csv, .tsv, and .fwf files.
                    # readxl,    for reading .xls, and .xlxs files.
                    # feather,   for sharing with Python and other languages.
                    # haven,     for SPSS, SAS and Stata files.
                    # httr,      for web apis.
                    # jsonlite   for JSON.
                    # rvest,     for web scraping.
                    # xml2,      for XML.
                    # modelr,    for modelling within a pipeline
                    # broom,     for turning models into tidy data
                    # hms,       for times.

library(magrittr)   # Pipeline operator
library(lobstr)     # Visualizing abstract syntax trees, stack trees, and object sizes
library(pander)     # Exporting/converting complex pandoc documents, EX: df to Pandoc table
library(ggforce)    # More plot functions on top of ggplot2
library(ggpubr)     # Automatically add p-values and significance levels  plots. 
                    # Arrange and annotate multiple plots on the same page. 
                    # Change graphical parameters such as colors and labels.
library(sf)         # Geo-spatial vector manipulation: points, lines, polygons
library(kableExtra) # Generate 90 % of complex/advanced/self-customized/beautiful tables
library(latex2exp)  # Latex axis titles in ggplot2
library(ellipse)    # Simultaneous confidence interval region to check C.I. of 2 slope parameters
library(plotly)     # User interactive plots
library(ellipse)    # Simultaneous confidence interval region to check C.I. of 2 regressors
library(olsrr)      # Model selections 
library(leaps)      # Regression subsetting 
library(pls)        # Partial Least squares
library(MASS)       # LDA, QDA, OLS, Ridge Regression, Box-Cox, stepAIC, etc,.
library(e1071)      # Naive Bayesian Classfier,SVM, GKNN, ICA, LCA
library(class)      # KNN, SOM, LVQ
library(ROCR)       # Precision/Recall/Sensitivity/Specificity performance plot 
library(boot)       # LOOCV, Bootstrap,
library(caret)      # Classification/Regression Training, run ?caret::trainControl

set.seed(1)        # make random results reproducible

current_dir <- getwd()

if (!is.null(current_dir)) {
  setwd(current_dir)
  remove(current_dir)
}
```

---

## Instructions:

- Due date: March 21
- Submit a typed report.
- Do a good job

---

## Preamble :

Consider the wine quality dataset from https://archive.ics.uci.edu/ml/datasets/Wine+Quality. We will focus only on the data concerning white wines (and not red wines). Dichotomize the quality variable as good, which takes the value 1 if quality â‰¥ 7 and the value 0, otherwise. We will take good as response and all the 11 physiochemical characteristics of the wines in the data as predictors.

## Problem Statements:

Use 10-fold cross-validation for estimating the test error rates below and compute the estimates using caret package with seed set to 1234 before each computation.

(a) Fit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.
(b) Repeat (a) using logistic regression.
(c) Repeat (a) using LDA.
(d) Repeat (a) using QDA.
(e) Compare the results in (a)-(d). Which classifier would you recommend? Justify your answer.
